{"cells":[{"cell_type":"code","execution_count":null,"id":"64209a9d","metadata":{"id":"64209a9d"},"outputs":[],"source":["from bigdl.orca import init_orca_context, stop_orca_context"]},{"cell_type":"code","execution_count":null,"id":"4133c423","metadata":{"id":"4133c423","outputId":"c1670dff-8eae-46b2-e072-c64a9f1cb717"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n","2024-05-12 03:38:00.511130: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-12 03:38:06.007607: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2024-05-12 03:38:06.007630: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2024-05-12 03:38:19.141465: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2024-05-12 03:38:19.141722: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2024-05-12 03:38:19.141741: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n","WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n"]},{"ename":"RuntimeError","evalue":"Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4704/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0"]}],"source":["from bigdl.orca.learn.tf.estimator import Estimator\n","from bigdl.orca.learn.tf.keras import Sequential\n","from bigdl.orca.learn.tf.keras.layers import LSTM, Dense"]},{"cell_type":"code","execution_count":null,"id":"f79cda03","metadata":{"id":"f79cda03","outputId":"59d9b120-f681-4e66-b7c1-8f17ce5b4500"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'bigdl.orca.learn.tf2.keras'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4704/3575839397.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigdl.orca.learn.tf2.keras'"]}],"source":["from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2.keras import Sequential\n","from bigdl.orca.learn.tf2.keras.layers import LSTM, Dense"]},{"cell_type":"code","execution_count":null,"id":"b3e5c9d1","metadata":{"id":"b3e5c9d1"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"code","execution_count":null,"id":"82525936","metadata":{"id":"82525936","outputId":"5504cb9c-2ae4-400d-e961-075248de632c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing orca context\n","Current pyspark location is : /home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/pyspark/__init__.py\n","Start to getOrCreate SparkContext\n","pyspark_submit_args is:  --driver-class-path /home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/core/lib/all-2.4.0-20230420.050641-1.jar:/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/friesian/lib/bigdl-friesian-spark_2.4.6-2.4.0-jar-with-dependencies.jar:/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/orca/lib/bigdl-orca-spark_2.4.6-2.4.0-jar-with-dependencies.jar:/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_2.4.6-2.4.0-jar-with-dependencies.jar pyspark-shell \n","[main] WARN  org.apache.spark.util.Utils  - Your hostname, ojash-VMware-Virtual-Platform resolves to a loopback address: 127.0.1.1; using 192.168.186.129 instead (on interface ens33)\n","[main] WARN  org.apache.spark.util.Utils  - Set SPARK_LOCAL_IP if you need to bind to another address\n","[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["2024-05-12 03:40:11,518 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","2024-05-12 03:40:11,524 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","2024-05-12 03:40:11,526 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","2024-05-12 03:40:11,527 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","24-05-12 03:40:11 [Thread-4] INFO  Engine$:122 - Auto detect executor number and executor cores number\n","24-05-12 03:40:11 [Thread-4] INFO  Engine$:124 - Executor number is 1 and executor cores number is 4\n"]},{"name":"stderr","output_type":"stream","text":["\n","User settings:\n","\n","   KMP_AFFINITY=granularity=fine,compact,1,0\n","   KMP_BLOCKTIME=0\n","   KMP_SETTINGS=1\n","   OMP_NUM_THREADS=1\n","\n","Effective settings:\n","\n","   KMP_ABORT_DELAY=0\n","   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n","   KMP_ALIGN_ALLOC=64\n","   KMP_ALL_THREADPRIVATE=128\n","   KMP_ATOMIC_MODE=2\n","   KMP_BLOCKTIME=0\n","   KMP_CPUINFO_FILE: value is not defined\n","   KMP_DETERMINISTIC_REDUCTION=false\n","   KMP_DEVICE_THREAD_LIMIT=2147483647\n","   KMP_DISP_HAND_THREAD=false\n","   KMP_DISP_NUM_BUFFERS=7\n","   KMP_DUPLICATE_LIB_OK=false\n","   KMP_FORCE_REDUCTION: value is not defined\n","   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n","   KMP_FORKJOIN_BARRIER='2,2'\n","   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n","   KMP_FORKJOIN_FRAMES=true\n","   KMP_FORKJOIN_FRAMES_MODE=3\n","   KMP_GTID_MODE=3\n","   KMP_HANDLE_SIGNALS=false\n","   KMP_HOT_TEAMS_MAX_LEVEL=1\n","   KMP_HOT_TEAMS_MODE=0\n","   KMP_INIT_AT_FORK=true\n","   KMP_INIT_WAIT=2048\n","   KMP_ITT_PREPARE_DELAY=0\n","   KMP_LIBRARY=throughput\n","   KMP_LOCK_KIND=queuing\n","   KMP_MALLOC_POOL_INCR=1M\n","   KMP_NEXT_WAIT=1024\n","   KMP_NUM_LOCKS_IN_BLOCK=1\n","   KMP_PLAIN_BARRIER='2,2'\n","   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n","   KMP_REDUCTION_BARRIER='1,1'\n","   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n","   KMP_SCHEDULE='static,balanced;guided,iterative'\n","   KMP_SETTINGS=true\n","   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n","   KMP_STACKOFFSET=64\n","   KMP_STACKPAD=0\n","   KMP_STACKSIZE=4M\n","   KMP_STORAGE_MAP=false\n","   KMP_TASKING=2\n","   KMP_TASKLOOP_MIN_TASKS=0\n","   KMP_TASK_STEALING_CONSTRAINT=1\n","   KMP_TEAMS_THREAD_LIMIT=8\n","   KMP_TOPOLOGY_METHOD=all\n","   KMP_USER_LEVEL_MWAIT=false\n","   KMP_VERSION=false\n","   KMP_WARNINGS=true\n","   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n","   OMP_ALLOCATOR=omp_default_mem_alloc\n","   OMP_CANCELLATION=false\n","   OMP_DEFAULT_DEVICE=0\n","   OMP_DISPLAY_AFFINITY=false\n","   OMP_DISPLAY_ENV=false\n","   OMP_DYNAMIC=false\n","   OMP_MAX_ACTIVE_LEVELS=2147483647\n","   OMP_MAX_TASK_PRIORITY=0\n","   OMP_NESTED=false\n","   OMP_NUM_THREADS='1'\n","   OMP_PLACES: value is not defined\n","   OMP_PROC_BIND='intel'\n","   OMP_SCHEDULE='static'\n","   OMP_STACKSIZE=4M\n","   OMP_TARGET_OFFLOAD=DEFAULT\n","   OMP_THREAD_LIMIT=2147483647\n","   OMP_TOOL=enabled\n","   OMP_TOOL_LIBRARIES: value is not defined\n","   OMP_WAIT_POLICY=PASSIVE\n","   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n","\n"]},{"name":"stdout","output_type":"stream","text":["24-05-12 03:40:12 [Thread-4] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 16\n","[Thread-4] WARN  org.apache.spark.SparkContext  - Using an existing SparkContext; some configuration may not take effect.\n","24-05-12 03:40:12 [Thread-4] INFO  Engine$:461 - Find existing spark context. Checking the spark conf...\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.Sample\n","BigDLBasePickler registering: bigdl.dllib.utils.common  Sample\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.EvaluatedResult\n","BigDLBasePickler registering: bigdl.dllib.utils.common  EvaluatedResult\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JTensor\n","BigDLBasePickler registering: bigdl.dllib.utils.common  JTensor\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JActivity\n","BigDLBasePickler registering: bigdl.dllib.utils.common  JActivity\n","Successfully got a SparkContext\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 0:>                                                          (0 + 4) / 4]"]},{"name":"stdout","output_type":"stream","text":["24-05-12 03:40:14 [Executor task launch worker for task 1] INFO  Engine$:162 - Initializing JavaGatewayServer on executor-driver \n","24-05-12 03:40:14 [Executor task launch worker for task 1] INFO  Engine$:182 - JavaGatewayServer initialized\n"]},{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"data":{"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://192.168.186.129:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v2.4.6</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[4]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        "],"text/plain":["<SparkContext master=local[4] appName=pyspark-shell>"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")"]},{"cell_type":"code","execution_count":null,"id":"92d05810","metadata":{"id":"92d05810"},"outputs":[],"source":["def build_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(64, input_shape=input_shape))\n","    model.add(Dense(1))\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model"]},{"cell_type":"code","execution_count":null,"id":"26f55b42","metadata":{"id":"26f55b42"},"outputs":[],"source":["def prepare_data():\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100,  1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)"]},{"cell_type":"code","execution_count":null,"id":"78e95005","metadata":{"id":"78e95005","outputId":"fdb06aea-adfd-46d0-f3d5-d79b96389313"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:bigdl.orca.learn.tf2.ray_estimator:Please use load function of the estimator to load model when model_creator is None.\n","2024-05-12 03:43:59,505\tINFO services.py:1340 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://192.168.186.129:8265\u001b[39m\u001b[22m\n"]},{"name":"stdout","output_type":"stream","text":["{'node_ip_address': '192.168.186.129', 'raylet_ip_address': '192.168.186.129', 'redis_address': '192.168.186.129:6379', 'object_store_address': '/tmp/ray/session_2024-05-12_03-43-53_204782_4704/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2024-05-12_03-43-53_204782_4704/sockets/raylet', 'webui_url': '192.168.186.129:8265', 'session_dir': '/tmp/ray/session_2024-05-12_03-43-53_204782_4704', 'metrics_export_port': 58524, 'node_id': '4e241fd5e9d66c6732fc20933350a7649948ee2c836f77f91f64f720'}\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:04.859617: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:04.859220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:05.364893: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:05.364987: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:05.473553: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:05.473585: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:09.773111: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:09.773487: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:09.773516: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:09.888171: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:09.888284: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:09.888297: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m WARNING:tensorflow:From /home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py:337: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m Instructions for updating:\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:13.976399: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:13.984125: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:13.984165: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ojash-VMware-Virtual-Platform): /proc/driver/nvidia/version does not exist\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m WARNING:tensorflow:From /home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py:337: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m Instructions for updating:\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:13.976423: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:13.984044: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:13.984125: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ojash-VMware-Virtual-Platform): /proc/driver/nvidia/version does not exist\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:14.027329: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:14.027577: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:14.754844: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://192.168.186.129:54785\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:14.805356: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://192.168.186.129:52445\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:16.394878: I tensorflow/core/distributed_runtime/coordination/coordination_service.cc:502] /job:worker/replica:0/task:1 has connected to coordination service. Incarnation: 6564692281557029771\n","\u001b[2m\u001b[36m(Worker pid=5621)\u001b[0m 2024-05-12 03:44:16.561322: I tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:277] Coordination agent has successfully connected.\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:16.524221: I tensorflow/core/distributed_runtime/coordination/coordination_service.cc:502] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 15245479308902420742\n","\u001b[2m\u001b[36m(Worker pid=5622)\u001b[0m 2024-05-12 03:44:16.549283: I tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:277] Coordination agent has successfully connected.\n"]}],"source":["estimator = Estimator.from_keras(build_model=build_model, workers_per_node=2)"]},{"cell_type":"code","execution_count":null,"id":"73b81afb","metadata":{"id":"73b81afb"},"outputs":[],"source":["train_data, val_data = prepare_data()"]},{"cell_type":"code","execution_count":null,"id":"dd9503b5","metadata":{"id":"dd9503b5","outputId":"78af8cad-cd30-482c-ded9-a75976d66ad8"},"outputs":[{"ename":"RayTaskError(TypeError)","evalue":"\u001b[36mray::Worker.step()\u001b[39m (pid=5621, ip=192.168.186.129, repr=<bigdl.orca.learn.dl_cluster.Worker object at 0x77c2506bd8d0>)\n  File \"/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py\", line 369, in step\n    validation_steps=validation_steps)\n  File \"/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py\", line 82, in handle_datasets_train\n    train_dataset = data_creator(config, config[\"batch_size\"])\nTypeError: 'tuple' object is not callable","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRayTaskError(TypeError)\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4704/3099437410.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/ray_estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data, epochs, batch_size, verbose, callbacks, validation_data, class_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, data_config, feature_cols, label_cols)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             worker_stats = ray.get([self.remote_workers[i].step.remote(**params_list[i])\n\u001b[0;32m--> 311\u001b[0;31m                                     for i in range(self.num_workers)])\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0;31m# TensorFlow automatically synchronizes results on all the workers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# and thus only need to return the result of the first worker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1711\u001b[0m                     \u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump_object_store_memory_usage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayTaskError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1713\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_instanceof_cause\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1714\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1715\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRayTaskError(TypeError)\u001b[0m: \u001b[36mray::Worker.step()\u001b[39m (pid=5621, ip=192.168.186.129, repr=<bigdl.orca.learn.dl_cluster.Worker object at 0x77c2506bd8d0>)\n  File \"/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py\", line 369, in step\n    validation_steps=validation_steps)\n  File \"/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py\", line 82, in handle_datasets_train\n    train_dataset = data_creator(config, config[\"batch_size\"])\nTypeError: 'tuple' object is not callable"]},{"name":"stderr","output_type":"stream","text":["2024-05-12 03:47:08,034\tERROR worker.py:85 -- Unhandled error (suppress with RAY_IGNORE_UNHANDLED_ERRORS=1): \u001b[36mray::Worker.step()\u001b[39m (pid=5622, ip=192.168.186.129, repr=<bigdl.orca.learn.dl_cluster.Worker object at 0x790d87391b50>)\n","  File \"/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py\", line 369, in step\n","    validation_steps=validation_steps)\n","  File \"/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py\", line 82, in handle_datasets_train\n","    train_dataset = data_creator(config, config[\"batch_size\"])\n","TypeError: 'tuple' object is not callable\n"]}],"source":["estimator.fit(train_data, epochs=10, batch_size=32, validation_data=val_data)"]},{"cell_type":"code","execution_count":null,"id":"045f46a3","metadata":{"id":"045f46a3"},"outputs":[],"source":["X_test = np.random.rand(10, 10, 1)\n",""]},{"cell_type":"code","execution_count":null,"id":"b5e2db7c","metadata":{"id":"b5e2db7c","outputId":"3f67b307-5938-439b-8124-10f9cd46135d"},"outputs":[{"name":"stderr","output_type":"stream","text":["ERROR:bigdl.dllib.utils.log4Error:\n","\n","****************************Usage Error************************\n","Only xshards, Spark DataFrame or orca TF Dataset are supported for predict\n","ERROR:bigdl.dllib.utils.log4Error:\n","\n","****************************Call Stack*************************\n"]},{"ename":"RuntimeError","evalue":"Only xshards, Spark DataFrame or orca TF Dataset are supported for predict","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4704/2080544588.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/ray_estimator.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, data, batch_size, verbose, steps, callbacks, data_config, feature_cols, min_partition_num, output_cols)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m             invalidInputError(False,\n\u001b[0;32m--> 585\u001b[0;31m                               \u001b[0;34m\"Only xshards, Spark DataFrame or orca TF Dataset are supported \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m                               \"for predict\")\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/dllib/utils/log4Error.py\u001b[0m in \u001b[0;36minvalidInputError\u001b[0;34m(condition, errMsg, fixMsg)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcondition\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0moutputUserMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrMsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrMsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Only xshards, Spark DataFrame or orca TF Dataset are supported for predict"]}],"source":["predictions = estimator.predict(X_test)"]},{"cell_type":"code","execution_count":null,"id":"1dab9ddf","metadata":{"id":"1dab9ddf","outputId":"30fee05f-82df-4846-8896-578d012ae512"},"outputs":[{"ename":"NameError","evalue":"name 'predictions' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4704/1609798684.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'predictions' is not defined"]}],"source":["print(predictions)"]},{"cell_type":"code","execution_count":null,"id":"5c179d4d","metadata":{"id":"5c179d4d"},"outputs":[],"source":["stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"a74cf493","metadata":{"id":"a74cf493","outputId":"c8beefe5-07a9-48f9-feb2-a644e8bee7b8"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n"]},{"ename":"RuntimeError","evalue":"Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_4704/1516028896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_4704/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf.estimator import Estimator\n","from bigdl.orca.learn.tf.keras import Sequential\n","from bigdl.orca.learn.tf.keras.layers import LSTM, Dense\n","from bigdl.orca.data import tf_dataset\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(64, input_shape=input_shape))\n","    model.add(Dense(1))\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = tf_dataset.from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = tf_dataset.from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(build_model=build_model, workers_per_node=2)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"9ab58ccf","metadata":{"id":"9ab58ccf","outputId":"a75854d0-7d49-4911-e823-7a7be33ce6b9"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'bigdl.orca.learn.tf2.keras'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_8334/570294400.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigdl.orca.learn.tf2.keras'"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2.keras import Sequential\n","from bigdl.orca.learn.tf2.keras.layers import LSTM, Dense\n","from bigdl.orca.data.tf_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(64, input_shape=input_shape))\n","    model.add(Dense(1))\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(build_model=build_model, workers_per_node=2)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n"]},{"cell_type":"code","execution_count":null,"id":"a4361687","metadata":{"id":"a4361687","outputId":"ac106642-08c4-4a9b-9a7f-9d96989e74c4"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n","2024-05-12 04:09:53.970218: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","2024-05-12 04:09:54.183735: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","2024-05-12 04:09:54.183757: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","2024-05-12 04:09:56.495773: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","2024-05-12 04:09:56.496060: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","2024-05-12 04:09:56.496075: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n","WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n"]},{"ename":"RuntimeError","evalue":"Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_8334/1516028896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf.estimator import Estimator\n","from bigdl.orca.learn.tf.keras import Sequential\n","from bigdl.orca.learn.tf.keras.layers import LSTM, Dense\n","from bigdl.orca.data import tf_dataset\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(64, input_shape=input_shape))\n","    model.add(Dense(1))\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = tf_dataset.from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = tf_dataset.from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(build_model=build_model, workers_per_node=2)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"c4aeecae","metadata":{"id":"c4aeecae","outputId":"b4956c6d-f3bb-45d7-d0ed-b140235970e2"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'bigdl.orca.learn.tf2.keras'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/3241592614.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigdl.orca.learn.tf2.keras'"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2.keras import Sequential\n","from bigdl.orca.learn.tf2.keras.layers import LSTM, Dense\n","from bigdl.orca.data.tf_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(64, input_shape=input_shape))\n","    model.add(Dense(1))\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(build_model=build_model, workers_per_node=2)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"e3f6e162","metadata":{"id":"e3f6e162","outputId":"e62dffac-6398-4303-946a-c0110b0d8350"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'bigdl.orca.learn.tf2.model'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/4220989303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigdl.orca.learn.tf2.model'"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2.model import Model\n","from bigdl.orca.learn.tf2.layer import LSTM, Dense\n","from bigdl.orca.data.tf_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(input_shape):\n","    model = Model(inputs=Input(shape=input_shape))\n","    model.add(LSTM(64))\n","    model.add(Dense(1))\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_model(model_creator=build_model, workers_per_node=2)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"e5f083b4","metadata":{"id":"e5f083b4","outputId":"02b8f8c1-0596-4030-ac83-403b35bdc96c"},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'keras' from 'bigdl.orca.learn.tf2' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/__init__.py)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/2904497222.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'keras' from 'bigdl.orca.learn.tf2' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/__init__.py)"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2 import keras\n","from bigdl.orca.data.tf_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    model = keras.Sequential([\n","        keras.layers.LSTM(64, input_shape=(10, 1)),\n","        keras.layers.Dense(1)\n","    ])\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"f7ffd616","metadata":{"id":"f7ffd616","outputId":"2c8548cb-6c1f-4287-bda2-a45a0c185525"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'bigdl.orca.learn.tf2.keras'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/1524189303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigdl.orca.learn.tf2.keras'"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2.keras.layers import Input, LSTM, Dense\n","from bigdl.orca.learn.tf2.keras.models import Model\n","from bigdl.orca.data.tf_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    inputs = Input(shape=(10, 1))\n","    x = LSTM(64)(inputs)\n","    outputs = Dense(1)(x)\n","    model = Model(inputs=inputs, outputs=outputs)\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"0887ddeb","metadata":{"id":"0887ddeb","outputId":"fdb744ab-aec0-4428-ffc9-3223e2494d0c"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'bigdl.orca.learn.tf2.model'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/2655384574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryCrossEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigdl.orca.learn.tf2.model'"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2.model import Model\n","from bigdl.orca.learn.tf2.layer import LSTM, Dense\n","from bigdl.orca.learn.tf2.loss import BinaryCrossEntropy\n","from bigdl.orca.data.tf_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    model = Model(inputs=[None, 10, 1])\n","    model.add(LSTM(64))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer=\"adam\", loss=BinaryCrossEntropy())\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.randint(0, 2, size=(100, 1))\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.randint(0, 2, size=(20, 1))\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_model(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"77ef2049","metadata":{"id":"77ef2049"},"outputs":[],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n"]},{"cell_type":"code","execution_count":null,"id":"b0942c70","metadata":{"id":"b0942c70","outputId":"e2a042ec-f65e-4125-ecc1-ab4808c693c2"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n"]},{"ename":"RuntimeError","evalue":"Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_9561/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_9561/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0"]}],"source":["from bigdl.orca.learn.tf.estimator import Estimator\n","from bigdl.orca.learn.tf.keras import Sequential\n","from bigdl.orca.learn.tf.keras.layers import LSTM, Dense"]},{"cell_type":"code","execution_count":null,"id":"94b3c0ba","metadata":{"id":"94b3c0ba","outputId":"fa1d63c1-be60-455f-eff3-b3feed63053d"},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'Model' from 'bigdl.orca.learn.tf2' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/__init__.py)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/2380315223.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryCrossEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'Model' from 'bigdl.orca.learn.tf2' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/__init__.py)"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2 import Model\n","from bigdl.orca.learn.tf2.layer import LSTM, Dense\n","from bigdl.orca.learn.tf2.loss import BinaryCrossEntropy\n","from bigdl.orca.data.tf2_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    model = Model(inputs=[None, 10, 1])\n","    model.add(LSTM(64))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer=\"adam\", loss=BinaryCrossEntropy())\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.randint(0, 2, size=(100, 1))\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.randint(0, 2, size=(20, 1))\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_model(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n"]},{"cell_type":"code","execution_count":null,"id":"80b578e8","metadata":{"id":"80b578e8","outputId":"1e080439-bac1-420c-e404-8ba9d5bfa64d"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:root:TF1 API has been deprecated, you may use TF2 Estimator instead.\n"]},{"ename":"RuntimeError","evalue":"Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/1516028896.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_9561/2811445506.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryCrossEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_9561/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_9561/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_9561/148871274.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf/estimator.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparkXShards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf1_data\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspark_estimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mSparkEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/tf/tf1_data.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mTF1Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     def __init__(self,\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/tfpark/__init__.py\u001b[0m in \u001b[0;36merror_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0merror_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;34m\"\"\"(nearly) all magic methods will be set to this function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mUnusableClass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Currently BigDL TFPark only supports TensorFlow 1.15.0, but your current TensorFlow installation is 2.11.0"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf.estimator import Estimator\n","from bigdl.orca.learn.tf.keras import Sequential\n","from bigdl.orca.learn.tf.keras.layers import LSTM, Dense\n","from bigdl.orca.data import tf_dataset\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(input_shape):\n","    model = Sequential()\n","    model.add(LSTM(64, input_shape=input_shape))\n","    model.add(Dense(1))\n","    model.compile(optimizer=\"adam\", loss=\"mse\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.rand(100, 1)\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.rand(20, 1)\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = tf_dataset.from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = tf_dataset.from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(build_model=build_model, workers_per_node=2)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"bafbc07a","metadata":{"id":"bafbc07a","outputId":"3110a554-e636-42bb-8077-4705aae84091"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'bigdl.orca.learn.tf2.model'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/2655384574.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBinaryCrossEntropy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bigdl.orca.learn.tf2.model'"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2.model import Model\n","from bigdl.orca.learn.tf2.layer import LSTM, Dense\n","from bigdl.orca.learn.tf2.loss import BinaryCrossEntropy\n","from bigdl.orca.data.tf_dataset import from_tensor_slices\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    model = Model(inputs=[None, 10, 1])\n","    model.add(LSTM(64))\n","    model.add(Dense(1, activation='sigmoid'))\n","    model.compile(optimizer=\"adam\", loss=BinaryCrossEntropy())\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.randint(0, 2, size=(100, 1))\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.randint(0, 2, size=(20, 1))\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_model(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"7e391610","metadata":{"id":"7e391610","outputId":"adca3dbc-193d-429b-fe29-0662ca2ae2fe"},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'TFDataset' from 'bigdl.orca.learn.tf2' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/__init__.py)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/2972899636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTFDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_into_bytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeserialize_from_bytes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'TFDataset' from 'bigdl.orca.learn.tf2' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/__init__.py)"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.learn.tf2 import TFDataset\n","from bigdl.orca.common.keras_utils import serialize_into_bytes, deserialize_from_bytes\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    model = Sequential([\n","        LSTM(64, input_shape=(10, 1)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n","    return serialize_into_bytes(model)\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.randint(0, 2, size=(100, 1))\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.randint(0, 2, size=(20, 1))\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = TFDataset.from_ndarrays(train_data, batch_size=batch_size)\n","    val_dataset = TFDataset.from_ndarrays(val_data, batch_size=batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict_ndarray(X_test)\n","print(predictions)\n"]},{"cell_type":"code","execution_count":null,"id":"1a150032","metadata":{"id":"1a150032","outputId":"90bb9a60-6bfe-452c-a87e-847dd109b963"},"outputs":[{"ename":"ImportError","evalue":"cannot import name 'tf_dataset' from 'bigdl.orca.data' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/__init__.py)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/3673205435.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minit_orca_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop_orca_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mbigdl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtf_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'tf_dataset' from 'bigdl.orca.data' (/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/data/__init__.py)"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from bigdl.orca.data import tf_dataset\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    model = Sequential([\n","        LSTM(64, input_shape=(10, 1)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.randint(0, 2, size=(100, 1))\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.randint(0, 2, size=(20, 1))\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = tf_dataset.from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = tf_dataset.from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n","\n","# Create an Estimator\n","estimator = Estimator.from_keras(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"bf328e9d","metadata":{"id":"bf328e9d","outputId":"1ac76cdd-de57-409e-8220-82574431d3ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["Initializing orca context\n","Current pyspark location is : /home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/pyspark/__init__.py\n","Start to getOrCreate SparkContext\n","pyspark_submit_args is:  --driver-class-path /home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/core/lib/all-2.4.0-20230420.050641-1.jar:/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/friesian/lib/bigdl-friesian-spark_2.4.6-2.4.0-jar-with-dependencies.jar:/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/orca/lib/bigdl-orca-spark_2.4.6-2.4.0-jar-with-dependencies.jar:/home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/share/dllib/lib/bigdl-dllib-spark_2.4.6-2.4.0-jar-with-dependencies.jar pyspark-shell \n","[main] WARN  org.apache.spark.util.Utils  - Your hostname, ojash-VMware-Virtual-Platform resolves to a loopback address: 127.0.1.1; using 192.168.186.129 instead (on interface ens33)\n","[main] WARN  org.apache.spark.util.Utils  - Set SPARK_LOCAL_IP if you need to bind to another address\n","[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"]},{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"]},{"name":"stdout","output_type":"stream","text":["[Thread-4] WARN  org.apache.spark.util.Utils  - Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n","2024-05-12 04:41:36,689 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","2024-05-12 04:41:36,692 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","2024-05-12 04:41:36,693 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","2024-05-12 04:41:36,694 Thread-4 WARN The bufferSize is set to 4000 but bufferedIo is false: false\n","24-05-12 04:41:36 [Thread-4] INFO  Engine$:122 - Auto detect executor number and executor cores number\n","24-05-12 04:41:36 [Thread-4] INFO  Engine$:124 - Executor number is 1 and executor cores number is 4\n"]},{"name":"stderr","output_type":"stream","text":["\n","User settings:\n","\n","   KMP_AFFINITY=granularity=fine,compact,1,0\n","   KMP_BLOCKTIME=0\n","   KMP_SETTINGS=1\n","   OMP_NUM_THREADS=1\n","\n","Effective settings:\n","\n","   KMP_ABORT_DELAY=0\n","   KMP_ADAPTIVE_LOCK_PROPS='1,1024'\n","   KMP_ALIGN_ALLOC=64\n","   KMP_ALL_THREADPRIVATE=128\n","   KMP_ATOMIC_MODE=2\n","   KMP_BLOCKTIME=0\n","   KMP_CPUINFO_FILE: value is not defined\n","   KMP_DETERMINISTIC_REDUCTION=false\n","   KMP_DEVICE_THREAD_LIMIT=2147483647\n","   KMP_DISP_HAND_THREAD=false\n","   KMP_DISP_NUM_BUFFERS=7\n","   KMP_DUPLICATE_LIB_OK=false\n","   KMP_FORCE_REDUCTION: value is not defined\n","   KMP_FOREIGN_THREADS_THREADPRIVATE=true\n","   KMP_FORKJOIN_BARRIER='2,2'\n","   KMP_FORKJOIN_BARRIER_PATTERN='hyper,hyper'\n","   KMP_FORKJOIN_FRAMES=true\n","   KMP_FORKJOIN_FRAMES_MODE=3\n","   KMP_GTID_MODE=3\n","   KMP_HANDLE_SIGNALS=false\n","   KMP_HOT_TEAMS_MAX_LEVEL=1\n","   KMP_HOT_TEAMS_MODE=0\n","   KMP_INIT_AT_FORK=true\n","   KMP_INIT_WAIT=2048\n","   KMP_ITT_PREPARE_DELAY=0\n","   KMP_LIBRARY=throughput\n","   KMP_LOCK_KIND=queuing\n","   KMP_MALLOC_POOL_INCR=1M\n","   KMP_NEXT_WAIT=1024\n","   KMP_NUM_LOCKS_IN_BLOCK=1\n","   KMP_PLAIN_BARRIER='2,2'\n","   KMP_PLAIN_BARRIER_PATTERN='hyper,hyper'\n","   KMP_REDUCTION_BARRIER='1,1'\n","   KMP_REDUCTION_BARRIER_PATTERN='hyper,hyper'\n","   KMP_SCHEDULE='static,balanced;guided,iterative'\n","   KMP_SETTINGS=true\n","   KMP_SPIN_BACKOFF_PARAMS='4096,100'\n","   KMP_STACKOFFSET=64\n","   KMP_STACKPAD=0\n","   KMP_STACKSIZE=4M\n","   KMP_STORAGE_MAP=false\n","   KMP_TASKING=2\n","   KMP_TASKLOOP_MIN_TASKS=0\n","   KMP_TASK_STEALING_CONSTRAINT=1\n","   KMP_TEAMS_THREAD_LIMIT=8\n","   KMP_TOPOLOGY_METHOD=all\n","   KMP_USER_LEVEL_MWAIT=false\n","   KMP_VERSION=false\n","   KMP_WARNINGS=true\n","   OMP_AFFINITY_FORMAT='OMP: pid %P tid %T thread %n bound to OS proc set {%a}'\n","   OMP_ALLOCATOR=omp_default_mem_alloc\n","   OMP_CANCELLATION=false\n","   OMP_DEFAULT_DEVICE=0\n","   OMP_DISPLAY_AFFINITY=false\n","   OMP_DISPLAY_ENV=false\n","   OMP_DYNAMIC=false\n","   OMP_MAX_ACTIVE_LEVELS=2147483647\n","   OMP_MAX_TASK_PRIORITY=0\n","   OMP_NESTED=false\n","   OMP_NUM_THREADS='1'\n","   OMP_PLACES: value is not defined\n","   OMP_PROC_BIND='intel'\n","   OMP_SCHEDULE='static'\n","   OMP_STACKSIZE=4M\n","   OMP_TARGET_OFFLOAD=DEFAULT\n","   OMP_THREAD_LIMIT=2147483647\n","   OMP_TOOL=enabled\n","   OMP_TOOL_LIBRARIES: value is not defined\n","   OMP_WAIT_POLICY=PASSIVE\n","   KMP_AFFINITY='noverbose,warnings,respect,granularity=fine,compact,1,0'\n","\n"]},{"name":"stdout","output_type":"stream","text":["24-05-12 04:41:37 [Thread-4] INFO  ThreadPool$:95 - Set mkl threads to 1 on thread 16\n","[Thread-4] WARN  org.apache.spark.SparkContext  - Using an existing SparkContext; some configuration may not take effect.\n","24-05-12 04:41:37 [Thread-4] INFO  Engine$:461 - Find existing spark context. Checking the spark conf...\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.Sample\n","BigDLBasePickler registering: bigdl.dllib.utils.common  Sample\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.EvaluatedResult\n","BigDLBasePickler registering: bigdl.dllib.utils.common  EvaluatedResult\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JTensor\n","BigDLBasePickler registering: bigdl.dllib.utils.common  JTensor\n","cls.getname: com.intel.analytics.bigdl.dllib.utils.python.api.JActivity\n","BigDLBasePickler registering: bigdl.dllib.utils.common  JActivity\n","Successfully got a SparkContext\n"]},{"name":"stderr","output_type":"stream","text":["[Stage 0:>                                                          (0 + 0) / 4]"]},{"name":"stdout","output_type":"stream","text":["24-05-12 04:41:41 [Executor task launch worker for task 0] INFO  Engine$:162 - Initializing JavaGatewayServer on executor-driver \n","24-05-12 04:41:41 [Executor task launch worker for task 0] INFO  Engine$:182 - JavaGatewayServer initialized\n"]},{"name":"stderr","output_type":"stream","text":["2024-05-12 04:41:53,345\tINFO services.py:1340 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://192.168.186.129:8266\u001b[39m\u001b[22m\n"]},{"name":"stdout","output_type":"stream","text":["{'node_ip_address': '192.168.186.129', 'raylet_ip_address': '192.168.186.129', 'redis_address': '192.168.186.129:47580', 'object_store_address': '/tmp/ray/session_2024-05-12_04-41-48_261716_9561/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2024-05-12_04-41-48_261716_9561/sockets/raylet', 'webui_url': '192.168.186.129:8266', 'session_dir': '/tmp/ray/session_2024-05-12_04-41-48_261716_9561', 'metrics_export_port': 41315, 'node_id': 'd92edc1784bf10c5df0022fb5aba324d2a4ed517c332bc0f9b5fc9c1'}\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:01.425430: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:01.644832: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:01.644864: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:02.734424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:02.734559: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:02.734572: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m WARNING:tensorflow:From /home/ojash/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/tf_runner.py:337: _CollectiveAllReduceStrategyExperimental.__init__ (from tensorflow.python.distribute.collective_all_reduce_strategy) is deprecated and will be removed in a future version.\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m Instructions for updating:\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m use distribute.MultiWorkerMirroredStrategy instead\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:03.906951: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:03.906976: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:03.906994: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ojash-VMware-Virtual-Platform): /proc/driver/nvidia/version does not exist\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:03.907617: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:03.925185: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:447] Started server with target: grpc://192.168.186.129:54111\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:03.930166: I tensorflow/core/distributed_runtime/coordination/coordination_service.cc:502] /job:worker/replica:0/task:0 has connected to coordination service. Incarnation: 6715375150683801501\n","\u001b[2m\u001b[36m(Worker pid=12084)\u001b[0m 2024-05-12 04:42:03.935012: I tensorflow/core/distributed_runtime/coordination/coordination_service_agent.cc:277] Coordination agent has successfully connected.\n"]},{"ename":"TypeError","evalue":"fit() got an unexpected keyword argument 'data_creator'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/731028105.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# Make predictions on new data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'data_creator'"]}],"source":["from bigdl.orca import init_orca_context, stop_orca_context\n","from bigdl.orca.learn.tf2.estimator import Estimator\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense\n","from tensorflow.data import Dataset\n","\n","import numpy as np\n","\n","# Initialize Orca context\n","init_orca_context(cluster_mode=\"local\", cores=4, memory=\"4g\")\n","\n","# Define the LSTM model architecture\n","def build_model(config):\n","    model = Sequential([\n","        LSTM(64, input_shape=(10, 1)),\n","        Dense(1, activation='sigmoid')\n","    ])\n","    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\")\n","    return model\n","\n","# Prepare your training data\n","def prepare_data():\n","    # Replace this with your own data preprocessing logic\n","    X_train = np.random.rand(100, 10, 1)\n","    y_train = np.random.randint(0, 2, size=(100, 1))\n","    X_val = np.random.rand(20, 10, 1)\n","    y_val = np.random.randint(0, 2, size=(20, 1))\n","    return (X_train, y_train), (X_val, y_val)\n","\n","# Define the data_creator function\n","def data_creator(config, batch_size):\n","    train_data, val_data = prepare_data()\n","    train_dataset = Dataset.from_tensor_slices(train_data).batch(batch_size)\n","    val_dataset = Dataset.from_tensor_slices(val_data).batch(batch_size)\n","    return train_dataset, val_dataset\n"]},{"cell_type":"code","execution_count":null,"id":"0cd56b54","metadata":{"id":"0cd56b54","outputId":"b5e03cab-5629-4cff-cda9-a5005f522f56"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2m\u001b[1m\u001b[36m(scheduler +1m0s)\u001b[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +1m0s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"]},{"name":"stderr","output_type":"stream","text":["2024-05-12 04:42:53,817\tWARNING worker.py:1245 -- The actor or task with ID ffffffffffffffffd7a766ffb267267c226e60d401000000 cannot be scheduled right now. You can ignore this message if this Ray cluster is expected to auto-scale or if you specified a runtime_env for this actor or task, which may take time to install.  Otherwise, this is likely due to all cluster resources being claimed by actors. To resolve the issue, consider creating fewer actors or increasing the resources available to this Ray cluster.\n","Required resources for this actor or task: {CPU: 4.000000}\n","Available resources on this node: {0.000000/4.000000 CPU, 131407920.068359 GiB/131407920.068359 GiB memory, 65703959.960938 GiB/65703959.960938 GiB object_store_memory, 1.000000/1.000000 node:192.168.186.129}\n"," In total there are 0 pending tasks and 1 pending actors on this node.\n"]},{"name":"stdout","output_type":"stream","text":["\u001b[2m\u001b[1m\u001b[33m(scheduler +1m35s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +2m10s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +2m45s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +3m20s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +3m55s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +4m30s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +5m5s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +5m41s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n","\u001b[2m\u001b[1m\u001b[33m(scheduler +6m16s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/4190413238.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_creator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/estimator.py\u001b[0m in \u001b[0;36mfrom_keras\u001b[0;34m(model_creator, config, verbose, workers_per_node, compile_args_creator, backend, cpu_binding, log_to_driver, model_dir, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers_per_node\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers_per_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                         \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_args_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile_args_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                                         cpu_binding=cpu_binding)\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spark\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcpu_binding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/ray_estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_creator, compile_args_creator, config, verbose, backend, workers_per_node, cpu_binding)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mworker_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTFRunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mworker_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mcpu_binding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_binding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             )\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/dl_cluster.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_workers, worker_cores, worker_cls, worker_param, cpu_binding)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             ray.get([worker.disable_cpu_affinity.remote(self.worker_cores)\n\u001b[0;32m--> 113\u001b[0;31m                      for worker in self.remote_workers])\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m         values, debugger_breakpoint = worker.get_objects(\n\u001b[0;32m-> 1707\u001b[0;31m             object_refs, timeout=timeout)\n\u001b[0m\u001b[1;32m   1708\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0;32m--> 354\u001b[0;31m             object_refs, self.current_task_id, timeout_ms)\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_metadata_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","# Create an Estimator\n","estimator = Estimator.from_keras(model_creator=build_model)\n","\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]},{"cell_type":"code","execution_count":null,"id":"361fd478","metadata":{"id":"361fd478","outputId":"9d7c1353-2faf-4a45-9d8d-6fa53f830f83"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2m\u001b[1m\u001b[33m(scheduler +6m51s)\u001b[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 4.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_9561/413578091.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEstimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_keras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbuild_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/estimator.py\u001b[0m in \u001b[0;36mfrom_keras\u001b[0;34m(model_creator, config, verbose, workers_per_node, compile_args_creator, backend, cpu_binding, log_to_driver, model_dir, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m                                         \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers_per_node\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers_per_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                                         \u001b[0mbackend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile_args_creator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile_args_creator\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                                         cpu_binding=cpu_binding)\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"spark\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcpu_binding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/tf2/ray_estimator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_creator, compile_args_creator, config, verbose, backend, workers_per_node, cpu_binding)\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mworker_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTFRunner\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0mworker_param\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                 \u001b[0mcpu_binding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcpu_binding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             )\n\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/bigdl/orca/learn/dl_cluster.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_workers, worker_cores, worker_cls, worker_param, cpu_binding)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             ray.get([worker.disable_cpu_affinity.remote(self.worker_cores)\n\u001b[0;32m--> 113\u001b[0;31m                      for worker in self.remote_workers])\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_workers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout)\u001b[0m\n\u001b[1;32m   1705\u001b[0m         \u001b[0;31m# TODO(ujvl): Consider how to allow user to retrieve the ready objects.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1706\u001b[0m         values, debugger_breakpoint = worker.get_objects(\n\u001b[0;32m-> 1707\u001b[0;31m             object_refs, timeout=timeout)\n\u001b[0m\u001b[1;32m   1708\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRayError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/bigdl/lib/python3.7/site-packages/ray/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout)\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0mtimeout_ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         data_metadata_pairs = self.core_worker.get_objects(\n\u001b[0;32m--> 354\u001b[0;31m             object_refs, self.current_task_id, timeout_ms)\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mdebugger_breakpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mb\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_metadata_pairs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["\n","# Create an Estimator\n","estimator = Estimator.from_keras(model_creator=build_model)\n"]},{"cell_type":"code","execution_count":null,"id":"0bba3636","metadata":{"id":"0bba3636"},"outputs":[],"source":["\n","# Train the model\n","estimator.fit(data_creator=data_creator, epochs=10, batch_size=32)\n","\n","# Make predictions on new data\n","X_test = np.random.rand(10, 10, 1)\n","predictions = estimator.predict(X_test)\n","print(predictions)\n","\n","# Stop Orca context\n","stop_orca_context()"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.16"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}